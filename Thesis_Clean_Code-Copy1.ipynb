{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096808a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49a28979",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Data Loading](#setup-and-data-loading)\n",
    "3. [Data Preprocessing](#data-preprocessing)\n",
    "4. [Sentiment Analysis](#sentiment-analysis)\n",
    "5. [Collaborative Filtering](#collaborative-filtering)\n",
    "6. [Content-Based Filtering](#content-based-filtering)\n",
    "7. [Hybrid Recommendation System](#hybrid-recommendation-system)\n",
    "8. [Results Compilation and Analysis](#results-compilation-and-analysis)\n",
    "9. [Performance Metrics](#performance-metrics)\n",
    "10. [Conclusions and Future Work](#conclusions-and-future-work)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b675",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This Jupyter Notebook documents the process of building a hybrid movie recommendation system. The system integrates collaborative and content-based filtering approaches, enhanced with sentiment analysis and natural language processing. Our objective is to explore various recommendation strategies and evaluate their performance based on a sample dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60222bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import gc  # Garbage Collector interface\n",
    "\n",
    "# Load the data\n",
    "movies = pd.read_csv('movies.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d6755",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "In this section, we perform initial data cleaning, handle missing values, and prepare the data for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e709f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for further analysis if needed\n",
    "combined_data = pd.merge(movies, ratings, on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84599d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a3efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20adfa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "295642ed",
   "metadata": {},
   "source": [
    "## Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333db4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for genre-based recommendations\n",
    "def content_based_recommendations_genres(movie_index, cosine_sim, n=10):\n",
    "    sim_scores = list(enumerate(cosine_sim[movie_index]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:n+1]  # Exclude self\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return movies.iloc[movie_indices][['title', 'genres']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bed653f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 29.0 GiB for an array with shape (62423, 62423) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6740/1430456608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'(?u)\\\\b\\\\w+\\\\b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtfidf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'genres'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcosine_sim_genres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n\u001b[0m\u001b[0;32m   1189\u001b[0m                         dense_output=dense_output)\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n\u001b[0;32m    155\u001b[0m             and dense_output and hasattr(ret, \"toarray\")):\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 29.0 GiB for an array with shape (62423, 62423) and data type float64"
     ]
    }
   ],
   "source": [
    "# Convert genres to TF-IDF features and calculate cosine similarity matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movies['genres'])\n",
    "cosine_sim_genres = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for NER-based recommendations\n",
    "nlp = spacy.load('en_core_web_sm')  # Load SpaCy English model\n",
    "def extract_named_entities(title):\n",
    "    doc = nlp(title)\n",
    "    return ' '.join([ent.text for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e92d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NER function to the movies dataset\n",
    "temp_movies = movies.copy()\n",
    "temp_movies['title_entities'] = temp_movies['title'].apply(extract_named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert named entities to TF-IDF features and calculate cosine similarity matrix\n",
    "tfidf_vectorizer_entities = TfidfVectorizer()\n",
    "tfidf_matrix_entities = tfidf_vectorizer_entities.fit_transform(temp_movies['title_entities'])\n",
    "cosine_sim_entities = cosine_similarity(tfidf_matrix_entities, tfidf_matrix_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate recommendations based on NER\n",
    "def content_based_recommendations_entities(movie_index, cosine_sim, n=10):\n",
    "    sim_scores = list(enumerate(cosine_sim[movie_index]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:n+1]  # Exclude self\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return temp_movies.iloc[movie_indices][['title', 'title_entities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine genre and NER recommendations with weights\n",
    "def weighted_combined_recommendations(movie_index, cosine_sim_genres, cosine_sim_ner, movies, temp_movies, weight_genre=0.8, weight_ner=0.2, n=10):\n",
    "    # Genre-based recommendations\n",
    "    genre_indices = content_based_recommendations_genres(movie_index, cosine_sim_genres, n*2).index\n",
    "    \n",
    "    # NER-based recommendations\n",
    "    ner_indices = content_based_recommendations_entities(movie_index, cosine_sim_ner, n).index\n",
    "    \n",
    "    # Initialize dictionary to keep track of scores\n",
    "    combined_scores = {}\n",
    "    \n",
    "    # Assign scores based on genre similarity\n",
    "    for idx in genre_indices:\n",
    "        combined_scores[idx] = combined_scores.get(idx, 0) + weight_genre\n",
    "    \n",
    "    # Update scores based on NER similarity\n",
    "    for idx in ner_indices:\n",
    "        if idx in combined_scores:\n",
    "            # If the movie is already recommended by genre, enhance its score\n",
    "            combined_scores[idx] += weight_ner\n",
    "        else:\n",
    "            # If not, add it with the NER weight\n",
    "            combined_scores[idx] = weight_ner\n",
    "    \n",
    "    # Sort movies by combined score\n",
    "    sorted_scores = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select top n movies\n",
    "    top_indices = [idx for idx, score in sorted_scores[:n]]\n",
    "    \n",
    "    # Fetch top movies from the original movies DataFrame\n",
    "    top_movies = movies.loc[top_indices]\n",
    "    \n",
    "    return top_movies[['movieId', 'title', 'genres']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_index = 0  # Index of a sample movie\n",
    "\n",
    "# Recommendations based on genres\n",
    "genre_recommendations = content_based_recommendations_genres(movie_index, cosine_sim_genres)\n",
    "print(\"Recommendations based on genres:\")\n",
    "print(genre_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd937b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_index = 0  # Index of a sample movie from the original 'movies' DataFrame\n",
    "\n",
    "# Recommendations based on NER (using the temporary dataset)\n",
    "ner_recommendations = content_based_recommendations_entities(movie_index, cosine_sim_entities)\n",
    "print(\"Recommendations based on NER extracted from titles:\")\n",
    "print(ner_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_index = 0  # Index of a sample movie from the original 'movies' DataFrame\n",
    "\n",
    "# Generate weighted combined recommendations\n",
    "weighted_recommendations = weighted_combined_recommendations(movie_index, cosine_sim_genres, cosine_sim_entities, movies, temp_movies, n=10)\n",
    "print(\"Weighted combined recommendations (80% genre, 20% NER):\")\n",
    "for index, row in weighted_recommendations.iterrows():\n",
    "    print(f\"{row['title']} - Genres: {row['genres']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19715bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a79023b",
   "metadata": {},
   "source": [
    "Collaborative Filering with Seniment Analysis included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7654cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment categorization function\n",
    "def categorize_sentiment(rating):\n",
    "    if rating >= 4.0:\n",
    "        return 'positive'\n",
    "    elif rating <= 2.0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0775ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment categorization to ratings\n",
    "ratings['sentiment'] = ratings['rating'].apply(categorize_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Surprise dataset for collaborative filtering\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the collaborative filtering model using SVD\n",
    "trainset = data.build_full_trainset()\n",
    "algo = SVD()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVD recommendation function\n",
    "def svd_recommendations(user_id, movies, algo, n=10):\n",
    "    predictions = []\n",
    "    for movie_id in movies['movieId'].unique():\n",
    "        prediction = algo.predict(user_id, movie_id)\n",
    "        predictions.append((movie_id, prediction.est))\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_movie_ids = [movie_id for movie_id, _ in predictions[:n]]\n",
    "    top_recommendations = [(movies[movies['movieId'] == movie_id]['title'].iloc[0], score) for movie_id, score in predictions[:n]]\n",
    "    return top_recommendations\n",
    "\n",
    "# Define a function to train the RandomForestClassifier for sentiment analysis\n",
    "def train_classifier_in_batches(data, batch_size=10000):\n",
    "    classifier = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "    for start in range(0, data.shape[0], batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_data = data.iloc[start:end]\n",
    "        classifier.fit(batch_data[['userId', 'movieId']], batch_data['sentiment'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23caa8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier in batches\n",
    "classifier = train_classifier_in_batches(ratings[['userId', 'movieId', 'sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Recommendations Function\n",
    "def hybrid_recommendation_with_sentiment(user_id, movies, algo, classifier, n=10):\n",
    "    svd_predictions = []\n",
    "    for movie_id in movies['movieId'].unique():\n",
    "        prediction = algo.predict(uid=str(user_id), iid=str(movie_id))\n",
    "        svd_predictions.append((movie_id, prediction.est))\n",
    "    svd_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_svd_movie_ids = [movie_id for movie_id, _ in svd_predictions[:n]]\n",
    "\n",
    "    adjusted_scores = []\n",
    "    for movie_id in top_svd_movie_ids:\n",
    "        predicted_sentiment = classifier.predict([[user_id, movie_id]])[0]\n",
    "        original_score = next(score for mid, score in svd_predictions if mid == movie_id)\n",
    "        adjusted_score = original_score + 0.1 if predicted_sentiment == 'positive' else original_score - 0.1\n",
    "        adjusted_scores.append((movie_id, adjusted_score))\n",
    "\n",
    "    adjusted_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_recommendations = [(movies[movies['movieId'] == movie_id]['title'].iloc[0], score) for movie_id, score in adjusted_scores[:n]]\n",
    "    return top_recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e39c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_id = random.randint(0, 7000)  # Random user ID for demonstration\n",
    "\n",
    "# SVD Recommendations\n",
    "top_svd_recommendations = svd_recommendations(user_id, movies, algo, n=10)\n",
    "print(\"Top recommendations based on SVD:\")\n",
    "for movie_title, estimated_rating in top_svd_recommendations:\n",
    "    print(f\"Movie Title: {movie_title}, Estimated Rating: {estimated_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# user_id = random.randint(65001, 65011)  # Random user ID for demonstration\n",
    "\n",
    "# Hybrid Recommendations\n",
    "top_hybrid_recommendations = hybrid_recommendation_with_sentiment(user_id, movies, algo, classifier, n=10)\n",
    "print(\"\\nTop recommendations based on Hybrid model with sentiment adjustment:\")\n",
    "for movie_title, score in top_hybrid_recommendations:\n",
    "    print(f\"Movie Title: {movie_title}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364deff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#experiment to see if random sentiment score included to SVD. Under preview and can be ignored \n",
    "def hybrid_recommendation_with_sentiment1(user_id, movies, algo, n=10):\n",
    "    # Generate SVD recommendations\n",
    "    svd_predictions = svd_recommendations(user_id, movies, algo, n)\n",
    "\n",
    "    # Adjust recommendations based on sentiment\n",
    "    adjusted_recommendations = []\n",
    "    for movie_id, svd_score in svd_predictions:\n",
    "        # Simulate sentiment score retrieval\n",
    "        sentiment_score = np.random.rand()\n",
    "        sentiment_weight = 0.8 + (sentiment_score * 0.4)\n",
    "        adjusted_score = svd_score * sentiment_weight\n",
    "        adjusted_recommendations.append((movie_id, adjusted_score))\n",
    "\n",
    "    # Sort by adjusted scores and select top N recommendations\n",
    "    adjusted_recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_adjusted_recommendations = adjusted_recommendations[:n]\n",
    "\n",
    "#     # Fetch movie titles for the top recommendations\n",
    "#     top_recommendations = []\n",
    "#     for movie_id, score in top_adjusted_recommendations:\n",
    "#         filtered_movies = movies[movies['movieId'] == movie_id]\n",
    "#         if not filtered_movies.empty:\n",
    "#             movie_title = filtered_movies['title'].iloc[0]\n",
    "#             top_recommendations.append((movie_title, score))\n",
    "\n",
    "    return top_adjusted_recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da84b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# user_id = random.randint(1, 7000)  # Assuming user IDs are between 1 and 610\n",
    "top_hybrid_recommendations = hybrid_recommendation_with_sentiment1(user_id, movies, algo, n=10)\n",
    "\n",
    "print(\"Top recommendations based on Hybrid model with sentiment adjustment:\")\n",
    "for movie_title, score in top_hybrid_recommendations:\n",
    "    print(f\"Movie Title: {movie_title}, Adjusted Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef61397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compile all recommendation results\n",
    "def compile_recommendation_results(user_id, movie_idx, movies, algo, classifier, cosine_sim_genres, cosine_sim_entities, n=10):\n",
    "    results = pd.DataFrame(index=range(n), columns=[\n",
    "         'Genre_Based', 'NER_Based', 'Weighted_Genre_NER', 'Final_Hybrid','SVD', 'SVD_Sentiment'\n",
    "    ])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Genre-Based Recommendations\n",
    "    genre_based_results = content_based_recommendations_genres(movie_idx, cosine_sim_genres, n)\n",
    "    results['Genre_Based'] = genre_based_results['title'].tolist()\n",
    "    \n",
    "    # NER-Based Recommendations\n",
    "    ner_based_results = content_based_recommendations_entities(movie_idx, cosine_sim_entities, n)\n",
    "    results['NER_Based'] = ner_based_results['title'].tolist()\n",
    "    \n",
    "    # Weighted Genre + NER Recommendations\n",
    "    weighted_genre_ner_results = weighted_combined_recommendations(movie_idx, cosine_sim_genres, cosine_sim_entities, movies, temp_movies, n=n)\n",
    "    results['Weighted_Genre_NER'] = weighted_genre_ner_results['title'].tolist()\n",
    "    \n",
    "    # SVD Recommendations\n",
    "    svd_results = svd_recommendations(user_id, movies, algo, n)\n",
    "    results['SVD'] = [r[0] for r in svd_results]\n",
    "    \n",
    "    # SVD with Sentiment Adjustments\n",
    "    svd_sentiment_results = hybrid_recommendation_with_sentiment(user_id, movies, algo, classifier, n)\n",
    "    results['SVD_Sentiment'] = [r[0] for r in svd_sentiment_results]\n",
    "    \n",
    "    # Final Hybrid Recommendations\n",
    "    # This is a conceptual step that could potentially be based on a more complex logic\n",
    "    # For simplicity, we'll take an average of the SVD scores and the content-based scores for each movie\n",
    "    final_hybrid_scores = {movie_id: (svd_score + content_score) / 2\n",
    "                           for movie_id, svd_score, content_score in zip(\n",
    "                               weighted_genre_ner_results['movieId'],\n",
    "                               [r[1] for r in svd_results],\n",
    "                               range(1, n+1)  # Mock scores for content-based recommendations\n",
    "                           )}\n",
    "    # Sort movies by the final hybrid score\n",
    "    sorted_final_hybrid = sorted(final_hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Fetch the top n recommendations\n",
    "    final_hybrid_top_n = [movies[movies['movieId'] == movie_id]['title'].iloc[0] for movie_id, _ in sorted_final_hybrid][:n]\n",
    "    results['Final_Hybrid'] = final_hybrid_top_n\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1040e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_id = random.randint(0, 7400)  # Random user ID for demonstration\n",
    "movie_idx = 0  # Index for \"Toy Story\" in the movies DataFrame\n",
    "\n",
    "# Assuming cosine_sim_genres and cosine_sim_entities are already defined from Part 1\n",
    "recommendation_results = compile_recommendation_results(user_id, movie_idx, movies, algo, classifier, cosine_sim_genres, cosine_sim_entities, n=10)\n",
    "recommendation_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27897a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
